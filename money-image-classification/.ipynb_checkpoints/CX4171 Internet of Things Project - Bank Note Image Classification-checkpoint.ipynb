{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "### Bank Note Image Classification Model Deployment\n",
    "*By Siek Ming Jun - U1820369F*\n",
    "\n",
    "For my project, I have trained an image recognition model to identify the bank notes available in Singapore using Google's **Teachable Machine platform**. The bank notes we would be training the model with are the **2, 5, 10 and 50** dollars. Multiple photos were taken in order to increase the accuracy of the model to identify the bank notes.\n",
    "\n",
    "Once the model has been trained, we exported the model as Tensorflow in the Keras format. With the model created, we are now able to deploy the model into **Azure Machine Learning** platform using purely python code inside this Jupyter notebook.\n",
    "\n",
    "This notebook provides details on how to deploy your machine learning model onto the Azure Machine Learning platform in the cloud. The steps are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: The resource group doesn't exist or was not provided. AzureML SDK is creating a resource group=iot-assignment in location=southeastasia using subscription=4a4f907e-ad41-435e-b51d-fe4a2d4c25f7.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying AppInsights with name iotwsinsightsa90d1b8486d.\n",
      "Deployed AppInsights with name iotwsinsightsa90d1b8486d. Took 3.81 seconds.\n",
      "Deploying KeyVault with name iotwskeyvaulte2f5cc058dd.\n",
      "Deploying StorageAccount with name iotwsstorage1e5117a6885e.\n",
      "Deployed KeyVault with name iotwskeyvaulte2f5cc058dd. Took 19.24 seconds.\n",
      "Deploying Workspace with name iot-ws.\n",
      "Deployed StorageAccount with name iotwsstorage1e5117a6885e. Took 20.94 seconds.\n",
      "Deployed Workspace with name iot-ws. Took 85.36 seconds.\n"
     ]
    }
   ],
   "source": [
    "# 1. Create Workspace in Azure \n",
    "#    You will need to enter your own Azure subscription ID into the arguments below\n",
    "\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.create(name='iot-ws', # provide a name for your workspace\n",
    "                      subscription_id='[subscription ID]', # provide your subscription ID\n",
    "                      resource_group='iot-assignment', # provide a resource group name\n",
    "                      create_resource_group=True,\n",
    "                      location='southeastasia') # For example: 'westeurope' or 'eastus2' or 'westus2' or 'southeastasia'.\n",
    "\n",
    "# write out the workspace details to a configuration file: .azureml/config.json\n",
    "ws.write_config(path='.azureml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1. If Workspace is already created, load Workspace from config \n",
    "# from azureml.core import Workspace\n",
    "\n",
    "# ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '/subscriptions/4a4f907e-ad41-435e-b51d-fe4a2d4c25f7/resourceGroups/iot-assignment/providers/Microsoft.MachineLearningServices/workspaces/iot-ws',\n",
       " 'name': 'iot-ws',\n",
       " 'identity': {'principal_id': '42b50a6e-e8e6-4c65-98a9-e73afaf86553',\n",
       "  'tenant_id': '15ce9348-be2a-462b-8fc0-e1765a9b204a',\n",
       "  'type': 'SystemAssigned'},\n",
       " 'location': 'southeastasia',\n",
       " 'type': 'Microsoft.MachineLearningServices/workspaces',\n",
       " 'sku': 'Basic',\n",
       " 'workspaceid': 'f7e3d26a-bdea-4265-aeeb-a3513de624a7',\n",
       " 'sdkTelemetryAppInsightsKey': '0855780c-10d5-4461-ae3c-9b15eb18c90d',\n",
       " 'description': '',\n",
       " 'friendlyName': 'iot-ws',\n",
       " 'creationTime': '2021-02-24T05:12:11.7875389+00:00',\n",
       " 'containerRegistry': '/subscriptions/4a4f907e-ad41-435e-b51d-fe4a2d4c25f7/resourceGroups/iot-assignment/providers/Microsoft.ContainerRegistry/registries/f7e3d26abdea4265aeeba3513de624a7',\n",
       " 'keyVault': '/subscriptions/4a4f907e-ad41-435e-b51d-fe4a2d4c25f7/resourcegroups/iot-assignment/providers/microsoft.keyvault/vaults/iotwskeyvaulte2f5cc058dd',\n",
       " 'applicationInsights': '/subscriptions/4a4f907e-ad41-435e-b51d-fe4a2d4c25f7/resourcegroups/iot-assignment/providers/microsoft.insights/components/iotwsinsightsa90d1b8486d',\n",
       " 'storageAccount': '/subscriptions/4a4f907e-ad41-435e-b51d-fe4a2d4c25f7/resourcegroups/iot-assignment/providers/microsoft.storage/storageaccounts/iotwsstorage1e5117a6885e',\n",
       " 'hbiWorkspace': False,\n",
       " 'allowPublicAccessWhenBehindVnet': False,\n",
       " 'imageBuildCompute': '',\n",
       " 'discoveryUrl': 'https://southeastasia.experiments.azureml.net/discovery',\n",
       " 'notebookInfo': {'fqdn': 'ml-iot-ws-southeastasia-f7e3d26a-bdea-4265-aeeb-a3513de624a7.notebooks.azure.net',\n",
       "  'resource_id': 'b3488bf0517c4e98bcc0e69de09969ec'}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. View details of workspace\n",
    "ws.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model bank_note_ic\n"
     ]
    }
   ],
   "source": [
    "# 3. Register model that you trained, model should be stored under './models/bank-note-model.h5'\n",
    "\n",
    "from azureml.core.model import Model\n",
    "# Tip: When model_path is set to a directory, you can use the child_paths parameter to include\n",
    "#      only some of the files from the directory\n",
    "model = Model.register(model_path = \"./models/bank-note-model.h5\",\n",
    "                       model_name = \"bank_note_ic\",\n",
    "                       description = \"Image Classification model trained outside Azure Machine Learning\",\n",
    "                       workspace = ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create entry script used in InferenceConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile score.py\n",
    "\n",
    "import tensorflow.keras\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import base64\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import glob\n",
    "from keras.models import load_model\n",
    "\n",
    "# Disable scientific notation for clarity\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Called when the deployed service starts\n",
    "def init():\n",
    "    global model\n",
    "\n",
    "    # Get the path where the deployed model can be found.\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), './bank-note-model.h5')\n",
    "    # load models\n",
    "    model = load_model(model_path)\n",
    "\n",
    "# Handle requests to the service\n",
    "def run(data):\n",
    "    try:\n",
    "        # Pick out the text property of the JSON request.\n",
    "        # This expects a request in the form of {\"text\": \"some text to score for sentiment\"}\n",
    "        data = json.loads(data)\n",
    "        prediction = predict(data['image_data'])\n",
    "        #Return prediction\n",
    "        return prediction\n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        return error\n",
    "\n",
    "def predict(image_data, include_neutral=True):\n",
    "    \n",
    "    note_classes = ['fifty_dollar', 'five_dollar', 'ten_dollar', 'two_dollar']\n",
    "    data = np.ndarray(shape=(1, 224, 224, 3), dtype=np.float32)\n",
    "    \n",
    "    imgdata = base64.b64decode(str(image_data))\n",
    "    image = Image.open(io.BytesIO(imgdata))\n",
    "\n",
    "    #resize the image to a 224x224 with the same strategy as in TM2:\n",
    "    #resizing the image to be at least 224x224 and then cropping from the center\n",
    "    size = (224, 224)\n",
    "    image = ImageOps.fit(image, size, Image.ANTIALIAS)\n",
    "    \n",
    "    #turn the image into a numpy array\n",
    "    image_array = np.asarray(image)\n",
    "\n",
    "    # Normalize the image\n",
    "    normalized_image_array = (image_array.astype(np.float32) / 127.0) - 1\n",
    "\n",
    "    # Load the image into the array\n",
    "    data[0] = normalized_image_array\n",
    "\n",
    "    # run the inference\n",
    "    prediction = model.predict(data)\n",
    "    return note_classes[prediction.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Define inference config\n",
    "\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Create the environment\n",
    "myenv = Environment(name=\"myenv\")\n",
    "conda_dep = CondaDependencies()\n",
    "\n",
    "# Define the packages needed by the model and scripts\n",
    "conda_dep.add_conda_package(\"tensorflow\")\n",
    "conda_dep.add_conda_package(\"numpy\")\n",
    "conda_dep.add_conda_package(\"scikit-learn\")\n",
    "# You must list azureml-defaults as a pip dependency\n",
    "conda_dep.add_pip_package(\"azureml-defaults\")\n",
    "conda_dep.add_pip_package(\"tensorflow\")\n",
    "conda_dep.add_pip_package(\"keras\")\n",
    "conda_dep.add_pip_package(\"pillow\")\n",
    "\n",
    "# Adds dependencies to PythonSection of myenv\n",
    "myenv.python.conda_dependencies=conda_dep\n",
    "\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\",\n",
    "                                   environment=myenv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running..................................................................................................................................................................................................\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n",
      "Healthy\n"
     ]
    }
   ],
   "source": [
    "# 6. Deploy to cloud\n",
    "\n",
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "from azureml.core.model import Model\n",
    "\n",
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1)\n",
    "service = Model.deploy(ws, \"aciservice\", [model], inference_config, deployment_config)\n",
    "service.wait_for_deployment(show_output = True)\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://302246c6-065b-4a25-8cf8-8e70627924aa.southeastasia.azurecontainer.io/score\n"
     ]
    }
   ],
   "source": [
    "# 7. Retrieve existing service\n",
    "# This will provide the url that your app can send a POST request to via REST \n",
    "\n",
    "service = Webservice.list(ws)[0]\n",
    "print(service.scoring_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running................................................................\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n",
      "Healthy\n",
      "2021-02-24T07:18:59,310546300+00:00 - nginx/run \n",
      "/usr/sbin/nginx: /azureml-envs/azureml_73af9c7ec19373116880cf0b602f9900/lib/libcrypto.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_73af9c7ec19373116880cf0b602f9900/lib/libcrypto.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_73af9c7ec19373116880cf0b602f9900/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_73af9c7ec19373116880cf0b602f9900/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_73af9c7ec19373116880cf0b602f9900/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "2021-02-24T07:18:59,321127500+00:00 - iot-server/run \n",
      "2021-02-24T07:18:59,323406600+00:00 - rsyslog/run \n",
      "2021-02-24T07:18:59,340847000+00:00 - gunicorn/run \n",
      "EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n",
      "2021-02-24T07:19:01,165468800+00:00 - iot-server/finish 1 0\n",
      "2021-02-24T07:19:01,170357900+00:00 - Exit code 1 is normal. Not restarting iot-server.\n",
      "Starting gunicorn 19.9.0\n",
      "Listening at: http://127.0.0.1:31311 (69)\n",
      "Using worker: sync\n",
      "worker timeout is set to 300\n",
      "Booting worker with pid: 99\n",
      "2021-02-24 07:19:04.667487: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /azureml-envs/azureml_73af9c7ec19373116880cf0b602f9900/lib:/azureml-envs/azureml_73af9c7ec19373116880cf0b602f9900/lib:\n",
      "2021-02-24 07:19:04.670443: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "SPARK_HOME not set. Skipping PySpark Initialization.\n",
      "Initializing logger\n",
      "2021-02-24 07:19:17,361 | root | INFO | Starting up app insights client\n",
      "2021-02-24 07:19:17,361 | root | INFO | Starting up request id generator\n",
      "2021-02-24 07:19:17,362 | root | INFO | Starting up app insight hooks\n",
      "2021-02-24 07:19:17,363 | root | INFO | Invoking user's init function\n",
      "2021-02-24 07:19:17.477600: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-02-24 07:19:17.483123: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /azureml-envs/azureml_73af9c7ec19373116880cf0b602f9900/lib:/azureml-envs/azureml_73af9c7ec19373116880cf0b602f9900/lib:\n",
      "2021-02-24 07:19:17.483221: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-02-24 07:19:17.483316: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (SandboxHost-637497477275076996): /proc/driver/nvidia/version does not exist\n",
      "2021-02-24 07:19:17.486277: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-02-24 07:19:17.493936: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "2021-02-24 07:19:22,784 | root | INFO | Users's init has completed successfully\n",
      "2021-02-24 07:19:22,787 | root | INFO | Skipping middleware: dbg_model_info as it's not enabled.\n",
      "2021-02-24 07:19:22,788 | root | INFO | Skipping middleware: dbg_resource_usage as it's not enabled.\n",
      "2021-02-24 07:19:22,789 | root | INFO | Scoring timeout is found from os.environ: 60000 ms\n",
      "2021-02-24 07:20:03,281 | root | INFO | Swagger file not present\n",
      "2021-02-24 07:20:03,281 | root | INFO | 404\n",
      "127.0.0.1 - - [24/Feb/2021:07:20:03 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "2021-02-24 07:20:32,700 | root | INFO | Swagger file not present\n",
      "2021-02-24 07:20:32,701 | root | INFO | 404\n",
      "127.0.0.1 - - [24/Feb/2021:07:20:32 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8. Update service: If service is already running, you can updated the service using the code below instead of redeploying \n",
    "#    the entire Workspace. This should not be run during the first deployment\n",
    "\n",
    "service.update(models=[model], inference_config=inference_config)\n",
    "service.wait_for_deployment(show_output=True)\n",
    "print(service.state)\n",
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Healthy\n",
      "http://302246c6-065b-4a25-8cf8-8e70627924aa.southeastasia.azurecontainer.io/score\n"
     ]
    }
   ],
   "source": [
    "# 9. Check the state of our service\n",
    "print(service.state)\n",
    "print(service.scoring_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST to url http://302246c6-065b-4a25-8cf8-8e70627924aa.southeastasia.azurecontainer.io/score\n",
      "prediction: \"ten_dollar\"\n"
     ]
    }
   ],
   "source": [
    "# 10. The code below is used to test whether the model we have deployed in the cloud is working.\n",
    "#    We send over a our test image file to our service and it will return the prediction to us.\n",
    "\n",
    "import base64\n",
    "import requests\n",
    "\n",
    "with open(\"test2.jpg\", \"rb\") as image_file:\n",
    "    image_data = base64.b64encode(image_file.read())\n",
    "\n",
    "image_data= image_data.decode('utf-8')\n",
    "\n",
    "input_data = {\"image_data\": str(image_data)}\n",
    "input_data = json.dumps(input_data)\n",
    "\n",
    "# input_data = \"{\\\"image_data\\\": \" + image_data + \"}\"\n",
    "\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "resp = requests.post(service.scoring_uri, input_data, headers=headers)\n",
    "\n",
    "print(\"POST to url\", service.scoring_uri)\n",
    "print(\"prediction:\", resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Once we are satisfied with our deployment, we are able to delete the service if we have no need for it\n",
    "#     in order to reduce cost. \n",
    "service.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. We can delete our model as well.\n",
    "model.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. We can also delete workspace and clean up resources\n",
    "ws.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "[[5.2672262e-05 1.1045220e-03 9.9774593e-01 1.0969235e-03]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"ten_dollar\"'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing of model on local machine without cloud\n",
    "\n",
    "# import tensorflow.keras\n",
    "# from PIL import Image, ImageOps\n",
    "# import numpy as np\n",
    "# import base64\n",
    "# import os\n",
    "# import json\n",
    "# import io\n",
    "# import base64\n",
    "\n",
    "# data = np.ndarray(shape=(1, 224, 224, 3), dtype=np.float32)\n",
    "    \n",
    "# model = tensorflow.keras.models.load_model('models/bank-note-model.h5')\n",
    "    \n",
    "# note_classes = ['fifty_dollar', 'five_dollar', 'ten_dollar', 'two_dollar']\n",
    "\n",
    "# with open(\"test2.jpg\", \"rb\") as image_file:\n",
    "#     image_data = base64.b64encode(image_file.read())\n",
    "# image_data = image_data.decode('utf-8')\n",
    "# image_data = str(image_data)\n",
    "\n",
    "# imgdata = base64.b64decode(str(image_data))\n",
    "# image = Image.open(io.BytesIO(imgdata))\n",
    "\n",
    "# #resize the image to a 224x224 with the same strategy as in TM2:\n",
    "# #resizing the image to be at least 224x224 and then cropping from the center\n",
    "# size = (224, 224)\n",
    "# image = ImageOps.fit(image, size, Image.ANTIALIAS)\n",
    "\n",
    "# #turn the image into a numpy array\n",
    "# image_array = np.asarray(image)\n",
    "\n",
    "# # Normalize the image\n",
    "# normalized_image_array = (image_array.astype(np.float32) / 127.0) - 1\n",
    "\n",
    "# # Load the image into the array\n",
    "# data[0] = normalized_image_array\n",
    "\n",
    "# # run the inference\n",
    "# prediction = model.predict(data)\n",
    "# print(prediction)\n",
    "# json.dumps(note_classes[prediction.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
